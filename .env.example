# Autofix LLM Provider Configuration
# Copy this file to .env and configure your preferred provider

# =============================================================================
# Provider Selection
# =============================================================================
# Choose which LLM provider to use: claude, openai, or ollama
# Can also be set via --provider CLI flag
AUTOFIX_PROVIDER=claude

# =============================================================================
# Claude Configuration (Anthropic)
# =============================================================================
# Get your API key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=sk-ant-api03-...

# Optional: Override default model (default: claude-sonnet-4)
# AUTOFIX_MODEL=claude-sonnet-4
# AUTOFIX_MODEL=claude-opus-4
# AUTOFIX_MODEL=claude-haiku-3.5

# =============================================================================
# OpenAI Configuration
# =============================================================================
# Get your API key from: https://platform.openai.com/api-keys
# OPENAI_API_KEY=sk-...

# Optional: Override default model (default: gpt-4)
# AUTOFIX_MODEL=gpt-4-turbo
# AUTOFIX_MODEL=gpt-4o
# AUTOFIX_MODEL=gpt-3.5-turbo

# Optional: Use OpenAI-compatible endpoints (Together.ai, Groq, Azure)
# AUTOFIX_API_BASE=https://api.together.xyz/v1
# AUTOFIX_API_BASE=https://api.groq.com/openai/v1
# AUTOFIX_API_BASE=https://<resource>.openai.azure.com/openai/deployments/<deployment>

# =============================================================================
# Ollama Configuration (Local Models)
# =============================================================================
# No API key required for local Ollama
# Start Ollama with: ollama serve
# Pull a model with: ollama pull llama2

# Optional: Override default model (default: llama2)
# AUTOFIX_MODEL=llama2
# AUTOFIX_MODEL=llama3
# AUTOFIX_MODEL=codellama
# AUTOFIX_MODEL=mistral
# AUTOFIX_MODEL=phi

# Optional: Override default endpoint (default: http://localhost:11434/v1)
# AUTOFIX_API_BASE=http://localhost:11434/v1

# =============================================================================
# Advanced Configuration (All Providers)
# =============================================================================

# Rate Limiting (tokens per minute)
# Claude default: 30000
# OpenAI default: 90000
# Ollama default: unlimited (0)
# AUTOFIX_RATE_LIMIT_TPM=30000

# Request timeout in seconds (default: 30)
# AUTOFIX_TIMEOUT_SECS=30

# Maximum retry attempts for failed requests (default: 3)
# AUTOFIX_MAX_RETRIES=3

# =============================================================================
# Usage Examples
# =============================================================================

# Example 1: Use Claude with default settings
# AUTOFIX_PROVIDER=claude
# ANTHROPIC_API_KEY=sk-ant-api03-...

# Example 2: Use OpenAI GPT-4 Turbo
# AUTOFIX_PROVIDER=openai
# OPENAI_API_KEY=sk-...
# AUTOFIX_MODEL=gpt-4-turbo

# Example 3: Use Together.ai (OpenAI-compatible)
# AUTOFIX_PROVIDER=openai
# OPENAI_API_KEY=your-together-api-key
# AUTOFIX_API_BASE=https://api.together.xyz/v1
# AUTOFIX_MODEL=mistralai/Mixtral-8x7B-Instruct-v0.1

# Example 4: Use local Ollama
# AUTOFIX_PROVIDER=ollama
# AUTOFIX_MODEL=codellama
# # No API key needed!

# =============================================================================
# CLI Override Examples
# =============================================================================
# You can override any setting via CLI flags:
#
# autofix --provider openai --model gpt-4-turbo --ios ...
# autofix --provider ollama --model mistral --verbose --ios ...
# autofix --provider claude --ios ...
